<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Wavefront Path Tracing</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->
    <!-- <meta property="og:image:type" content="image/png"> -->
    <!-- <meta property="og:image:width" content="1200"> -->
    <!-- <meta property="og:image:height" content="630"> -->
    <meta property="og:type" content="website">
    <!-- <meta property="og:url" content="https://dorverbin.github.io/refnerf"> -->
    <meta property="og:title" content="Wavefront Path Tracing: CMU 15-618 Final Project">
    <!-- <meta property="og:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing."> -->

    <!-- <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <!-- <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>Wavefront Path Tracing</b> <br> 
                <small>
                    CMU 15-618 Final Project
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            Fulun Ma
                            <br>(fulunm)
                        </td>
                        <td>
                            Benran Hu
                            <br>(benranh)
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="./file/proposal.pdf">
                                <h4><strong>Project Proposal</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./file/milestone.pdf">
                                <h4><strong>Milestone Report</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./file/final.pdf">
                                <h4><strong>Final Report</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Updated Schedule
                </h3>
                <ul>
                    <li>Nov 15 - Nov 21: <b>(Completed)</b> Get started on the CPU path tracer codebase. Start to implement the megakernel CUDA version. </li>
                    <li>Nov 21 - Nov 27: <b>(Completed)</b> Complete the megakernel version. Start implementing the wavefront version. </li>
                    <li>Nov 28 - Dec 3 <b>(Ongoing, milestone due)</b>: Complete the wavefront version. Explore different pipelines, optimizations, and design choices. </li>
                    <li>Dec 4 - Dec 7: Add wavefront design (pipelining) to BVH traversal, switch to "wider" BVH, and parallel ray-triangle intersection test. </li>
                    <li>Dec 8 - Dec 11: Add wavefront design to ray generation and material/shading evaluation. </li>
                    <li>Dec 12 - Dec 15: Test and profile different implementations on different scenes. Analyze the workload and performance. Prepare the final report and poster session. </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Work Done So Far
                </h3>
                <p class="text-justify">
                    We have completed implementing a CUDA version of an existing CPU path tracer using the naive megakernel approach, i.e., one CUDA thread is responsible for the whole procedure of ray generation, intersection tests, shading evaluation, ray extension, and image updating. All scene data (geometry, acceleration structures, lightings, materials, cameras) are stored in GPU global memory.
                    <br><br>
                    We also tried to directly use the CPU version of the BVH (i.e., a binary tree with large tree height and small leaf node size) in CUDA for ray-instance intersection, and a sequential ray-triangle intersection test in the leaf nodes. We measured some preliminary performance and compared to the CPU and the OptiX hardware accelerated version.            
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Goals and Deliverables
                </h3>
                <p class="text-justify">
                    We are slightly behind our previous schedule, but the next direction is clear and the overall progress is smooth. Hence, we are still confident that we can achieve the following goals and present the listed deliverables.
                    <br><br>
                    As for the "nice to have" goals in case we have extra time, we might not be able to try SIMD implementation on CPU, or implement a heterogeneous version using both CPU and GPU, but we may still try to accelerate BVH construction using GPU, or including BVH refitting for animated scenes. We may also try to incorporate the hardware accelerated ray-triangle intersection test into our pipeline.
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>
                </div>
			</div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Preliminary Results
                </h3>
                <p class="text-justify">
                    We ran our current megakernel version of the path tracer on several different scenes and compare its performance to the CPU version and the implementation using OptiX (thus NVIDIA's RT cores) hardware accelerated intersection tests. Note that the OptiX version uses the same megekernel design.                    

                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <img src="./img/dragon.jpg" width="100%">
                        </td>
                        <td align="left" valign="top" width="50%">
                            <img src="./img/dining-room.png" width="100%">
                        </td>
                    </tr>
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <img src="./img/kitchen.png" width="100%">
                        </td>
                        <td align="left" valign="top" width="50%">
                            <img src="./img/landscape.png" width="100%">
                        </td>
                    </tr>
                </table>
                <div class="text-center">
                    The scenes are from Benedikt Bitterli's <a href="https://benedikt-bitterli.me/resources/">rendering resources</a> 
                    and <a href="https://github.com/mmp/pbrt-v4-scenes#landscape">PBRT-v4's repo</a>.
                </div>
                <br>
                We use scenes of different geometry complexity and shading complexity (variety of materials and lightings). The <i>dragon</i> scene contains a single object with simple material, whereas the <i>landscape</i> contains 23,241 plant models and 3.1B triangles. The other two scenes sit inbetween and feature different sets of materials.
                <br><br>
                As this is simply the preliminary results to get a sense of the performance bottleneck and optimization direction, we only time the total rendering time for rendering 1 sample-per-pixel (spp) for a 1280x720 image, and average the rendering time over 64 samples.
                <br><br>
                <table style="border-collapse:collapse;border-spacing:0;margin:0px auto" class="tg"><thead><tr><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Time (s)</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Dragon</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">The Breakfast Room</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Country Kitchen</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:bold;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">Landscape</th></tr></thead><tbody><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">CPU</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0811</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.4537</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.6665</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">5.7601</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">CUDA</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0180</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.1346</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.1292</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">2.7245</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">OptiX</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0076</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0203</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0243</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">0.0888</td></tr></tbody></table>

                <br><br>
                From the table, we can see that even with the naive megakernel design, the CUDA implementation can still achieves 2x-5x speedup compared to the CPU implementation. However, considering that path tracing is massively parallelizable, this speedup is clearly unsatisfactory, and the main reason is the extremely high divergence occured during BVH traversal, ray extension, and shading evaluation.
                <br><br>
                Comparing the performance of CUDA and of OptiX implementation, we found that the performance of ray-scene intersection test is crucial for the overall rendering performance, especially in scenes with highly complex geometries. Therefore, the first optimization we plan to implement for our wavefront version is to make the BVH traversal a separate pipeline stage. We need to make the BVH "wider" by increasing the fan-out width and decreasing the tree height, which reduces the costly pointer chasing and allows us to parallelize ray-box/ray-triangle intersection test over all threads within a group. We do not expect that this can reach the same speedup as the OptiX version, but it will be a significant optimization to consider first.
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Summary
                </h3>
                <p class="text-justify">
                    We are going to implement a parallel path tracer using a specific optimization technique named wavefront path tracing in CUDA. We will compare its performance to simpler parallel implementation using megakernels on CPU and GPU, and analyze how ray path and shader divergence affect parallelization performance.                </p>
            </div>
        </div>

        <!-- <image src="img/architecture.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;"> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Background
                </h3>
                <div class="text-justify">
                    Path tracing, as one branch of the ray tracing methods, can potentially benefit from parallelization greatly by distributing the rays to different CPU threads using OpenMP or MPI. However, on GPUs or CPUs with SIMD support, naively assigning rays to each CUDA thread or SIMD lane can result in very poor speedup, given the highly divergent workload among rays.                     
                    <br><br>
                    
                </div>
                <div class="text-justify">
                    A simplistic approach to parallelizing path tracing with CUDA or SIMD involves creating "megakernels" or "ray packets" that process an entire light path from beginning to end. However, this method faces challenges as rays in different threads might end at various points, encounter diverse object types, or interact with objects using different shaders. This can cause significant divergence within GPU warps since each thread might execute distinct instructions, leading to idle threads and low warp utilization. Additionally, when accessing acceleration structures like Bounding Volume Hierarchies (BVHs), the divergence in ray paths can adversely affect cache coherence.
                </div> <br>
                <div class="text-justify">
                    In contrast, wavefront path tracing divides the process into several smaller kernels. Each kernel either advances the path to the next kernel, or terminates it. This allows opportunities to sort the rays by their origins, directions, hit surface types, or other properties, and compact the thread blocks when some rays terminate early, so that we can achieve better execution coherence. Smaller kernels also allow pipelining (streaming) and finer-scale work queueing, where we may get better work balancing. The idea is described in [1] in detail.
                </div> <br>

                <div class="text-center">
                    <img src="./img/wavefront.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    Above is an illustration of one possible implemention. We may divide a "wave" in a ray path into three stages. In the logic stage we accumulate the shading result, perform sampling, determine whether the ray should be terminated, and determine the material type of the hit location. In the material stage we evaluate the material, and in the ray cast stage we extend the rays, which will form the next wave. We can use a work queue and maintain a pool of working threads for each kernel.
                </div>


                <!-- <div class="text-center">
                    <video id="refdir" width="40%" playsinline autoplay loop muted>
                        <source src="video/reflection_animation.mp4" type="video/mp4" />
                    </video>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Challenges
                </h3>
                <div class="text-justify">
                    As mentioned above, parallelizing path tracing with CUDA or SIMD is trivial, but getting maximum speedup is hard. Essentially, path tracing in practice is _not_ a nice data parallel program, and there can be high divergence if we directly map each ray to a CUDA thread. This is the main challenge for our project, and also the main motivation of using a wavefront design.                </div>
                
                <!-- <div class="text-center">
                    <img src="./img/ide.png" width="50%">
                </div> -->
                <br>
                <div class="text-justify">
                    While [1] gives one possible formulation of wavefront path tracing, we would like to note that the design space for this method is huge, and there is a lot of performance tradeoffs depending on the actual workload charateristics. We will describe some of the major challenges.                </div>

                <h4>
                    Execution Divergence
                </h4>
                <div class="text-justify">
                    The goal of the wavefront design is to reduce divergence in GPU warps or CPU SIMD groups, however, the design _alone_ does not resolve all the divergence issues. Within each kernel divergent execution and memory access might still exist, if we do not sort or reorder the rays. We also need to make tradeoffs between divergence and kernel granularity, as dividing kernels into very small pieces helps with coherence but adds overhead for communication and orchestration.
                </div>

                <h4>
                    Memory Access Locality
                </h4>
                <div class="text-justify">
                    As mentioned, divergent rays can lead to low spatial locality in BVH traversing, which can quickly overwhelm GPU's thread local caches and global memory bandwidth. This implies that ray sorting strategies may be needed to maximize the locality within each thread block. The spatial locality of transferring data from stage to stage should be exploited as well.                </div>

                <h4>
                    Space and Communication Overhead
                </h4>
                <div class="text-justify">
                    Wavefront design requires us to place all the data produced and consumed in each stage into global buffers. There can be a high space requirement for the buffers, and reading/writing them in each stage can be very time-consuming, lowering arithmetic intensity. Nonetheless these redundant data tranfers does not exist in megakernel design. We therefore need to carefully divide the stages and kernels to minimize the space and communication overhead.                </div>

                <h4>
                    Synchronization and Orchestration Overhead
                </h4>
                <div class="text-justify">
                    There are multiple ways to implement and orchestrate the wavefront design. Apart from the one shown in the diagram using work queues, we may also launch the kernels in each stage only after all work has been generated, which is simpler and reduces overhead in work fetching, but has higher overhead in launching threads. Alternatively, we can pipeline all the stages, or even stream data between them, which requires us to think about how to synchronize stages.
                </div>
                <br>
                <div class="text-justify">
                    If we decide to maintain a threadpool for each kernel, we then need to care about dynamically adjust the threadpool sizes based on the load. Additionally, there might be multiple consumers and producers writing to these buffers concurrently if we do streaming, thus we need proper synchronization for the queues and buffers.                </div>

                <h4>
                    Kernel and Pipeline Organization
                </h4>
                <div class="text-justify">
                    We need to determine the granularity of kernels and pipeline stages to make sure the gain outweighs the overhead. For instance, we do not want to create two separate kernels and queues for two shaders that are largely the same. Fine granularity of kernels may yield higher coherence, but also suffers from high communication, space, and orchestration overhead, and low arithmetic intensity. This requires us to carefully analyze the path tracing scenes we plan to render.                 </div>



                <!-- <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/ide_animation.mp4" type="video/mp4" />
                    </video>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Resources
                </h3>
                <!-- <div class="video-compare-container">
                    <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                </div> -->

                <div class="text-justify">
                    We plan to use the GHC machines with RTX 2080Ti as our main platform of development and experimentation. We will likely start with an existing pure CPU-based path tracing codebase (e.g., 
                    <a href="https://github.com/CMU-Graphics/Scotty3D">Scotty3D</a>, <a href="https://github.com/wjakob/nori">Nori</a>, 
                    and <a href="https://github.com/xelatihy/yocto-gl">Yocto/GL</a>), and adapt it to CUDA, so that we can reuse the scene I/O and user inferfaces, and spend most of our time implementing the wavefront pipeline. We will use the design in [1] and [2] as a reference, but we almost surely will explore and choose different designs and implementations.
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Goals and Deliverables
                </h3>

                <h4>
                    Goals Plan to Achieve
                </h4>
                <div class="text-justify">
                <ul>
                    <li>Implement two path tracers in CUDA using wavefront design and megakernel design, respectively. </li>
                    <li>Compare their performance and analyze how our wavefront design alleviate execution divergence, and how that affects the final performance. We target a speedup over 1.3x in simple scenes, and over 2x in more complex scenes, based on the results in [1].</li>
                    <li>Test under multiple scenes with different configurations to analyze when wavefront path tracing benefits the most.</li>
                    <li>Experiment with some basic ray sorting and work queueing methods and analyze and performance and tradeoffs.</li>
                </ul>

                In case of slow progress, we stick with the first three goals.
                </div>

                <h4>Goals Hope to Achieve</h4>
                <ul>
                    <li>Explore more complex design choices like streaming, threadpool, etc.</li>
                    <li>Implement wavefront path tracing also using SIMD on CPU and compare its performance to the megakernel version using SIMD, and without using SIMD.</li>
                    <li>Accelerate BVH construction and/or refitting using GPU.</li>
                    <li>Extend the path tracer to utilize heterogeneous architectures, e.g., offloading certain highly divergent rays to CPU, or using the RTX cores in the GPU for hardware acceleration.</li>
                </ul>

                <h4>Deliverables</h4>
                <ul>
                    <li>We may present an interactive demo to show the speedup of our design and how it varies with the scene, if our final implementation can achieve interactive framerates.</li>
                    <li>Speedup plots and other profiling metrics (warp utilization, cache hit rate, memory usage, etc.) of our wavefront design compared to the megakernel design on GPU, under different scene configurations.</li>
                    <li>Performance comparison and analysis of different design choices mentioned in the challenges section.</li>
                </ul>   

                <!-- Our method also produces accurate renderings and surface normals from captured photographs: -->
                <!-- <div class="video-compare-container" style="width: 100%">
                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                </div> -->
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Platform Choice
                </h3>

                <div class="text-justify">
                    We choose CUDA as the problem of divergence only exists in data parallel programming models, which the main rationale of wavefront design. It makes little sense to apply the same design to shared address space or message passing models, like using OpenMP or MPI on CPUs.
                    <br><br>
                    Although wavefront design also benefits SIMD execution on CPU, the degree of parallelism is much lower than GPU, thus the issue is less pronounced. Also, implementing ray casting and shading using SIMD intrinsics or ISPC might be more complex than implementing them in CUDA.
                    <br><br>
                    Overall, even though normal path tracing has high divergence and does not match well with the data parallel programming model, it is naturally parallelizable and can still enjoy the massive parallelism of GPU. In practice, a good wavefront implementation of path tracing on GPU is also much faster than a CPU version using OpenMP, hence it is interesting to see how we can adapt the workload to better suit the CUDA programming model.
                </div>

                <!-- <div style="overflow: hidden;">
                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">
                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                    </video>
                </div>

                <div class="text-justify">
                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:
                </div>
                
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/car_color3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table> -->

            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Schedule
                </h3>
                <ul>
                    <li>Nov 15 - Nov 21: Get started on the CPU path tracer codebase. Start to implement the megakernel CUDA version.</li>
                    <li>Nov 21 - Nov 27: Complete the megakernel version. Start implementing the wavefront version.</li>
                    <li>Nov 28 - Dec 3 (milestone due): Complete the wavefront version. Explore different pipelines, optimizations, and design choices.</li>
                    <li>Dec 4 - Dec 10: Test and profile different implementations on different scenes. Analyze the workload and performance.</li>
                    <li>Dec 11 - Dec 15: Prepare the final report and poster session.</li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    References
                </h3>
                <p class="text-justify">
                    [1] S. Laine, T. Karras, and T. Aila, “Megakernels considered harmful: Wavefront path tracing on
                    GPUs”, <i>Proceedings of the 5th High-Performance Graphics Conference</i>, pp. 137–143, 2013.
                    <br>
                    [2] A. Keller <i>et al.</i>, “The iray light transport simulation and rendering system”, <i>ACM SIGGRAPH 2017
                    Talks</i>, pp. 1–2, 2017.
                    <br> <br>
                    The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>